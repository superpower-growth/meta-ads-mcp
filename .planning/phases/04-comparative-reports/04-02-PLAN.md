---
phase: 04-comparative-reports
plan: 02
type: execute
---

<objective>
Complete Phase 4 with cross-entity comparison tool for competitive analysis within account.

Purpose: Enable Claude to answer questions like "Which campaign performed best?" and "How does Campaign A compare to Campaign B on video engagement?" by providing side-by-side performance comparison and ranking.

Output: compare-entities MCP tool that compares multiple campaigns/adsets/ads on selected metrics, ranks by performance, and identifies winners/losers across dimensions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-comparative-reports/04-01-SUMMARY.md

# Key files
@src/meta/metrics.ts
@src/lib/comparison.ts
@src/lib/parsers.ts
@src/tools/compare-time-periods.ts
@src/tools/index.ts
@src/index.ts

**Tech stack available:** MetricsService, comparison utilities, all parsers

**Established patterns:**
- Comparison utility for delta calculations
- MetricsService for insights queries
- Manual inputSchema for tools

**Constraining decisions:**
- Plan 04-01: Comparison utility with delta and classification functions
- Plan 02-02: Campaign level queries supported
- Plan 03-03: Video engagement metrics and scoring available

**Issues being addressed:** None
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create entity ranking utility</name>
  <files>src/lib/ranking.ts</files>
  <action>
Create utility module for ranking and comparing multiple entities.

Export functions:

1. rankByMetric(entities, metricName, direction = 'desc'):
   - Sort entities by specified metric
   - direction: 'desc' for metrics where higher is better, 'asc' for lower is better
   - Return sorted array with rank numbers (1, 2, 3, ...)
   - Handle ties: Same value = same rank

2. calculatePercentiles(entities, metricName):
   - For each entity, calculate what percentile it falls into
   - Return percentile (0-100) for quick relative positioning
   - Example: Entity at 75th percentile is better than 75% of others

3. identifyBestPerformers(entities, metrics):
   - For each metric, find top 3 entities
   - Return object: { [metricName]: [top3Entities] }
   - Useful for multi-metric comparison (best CTR ≠ best ROAS)

4. identifyWorstPerformers(entities, metrics):
   - Same as identifyBestPerformers but bottom 3
   - Helps identify underperformers needing optimization

5. calculateMetricStats(entities, metricName):
   - Calculate: min, max, mean, median, stdDev
   - Provides context for individual entity performance
   - Return stats object

Add TypeScript interfaces for ranking results.
Add JSDoc with examples.

Why separate module: Reusable ranking logic. Useful for campaign comparison, ad set comparison, and future leaderboard features.

What to avoid: Don't filter low-performers automatically (user wants full picture). Don't make assumptions about sample size (handle 2 entities or 200). Don't require minimum data points (even 1 entity can be "ranked").
  </action>
  <verify>npm run type-check passes, ranking functions exported, statistical calculations correct</verify>
  <done>Ranking utility module created with sorting, percentiles, top/bottom identification, and stats functions</done>
</task>

<task type="auto">
  <name>Task 2: Create compare-entities MCP tool</name>
  <files>src/tools/compare-entities.ts</files>
  <action>
Create MCP tool for cross-campaign/adset/ad performance comparison and ranking.

Zod schema:
```typescript
const schema = z.object({
  dateRange: z.enum(['last_7d', 'last_30d', 'last_90d', 'this_month']).default('last_7d'),
  level: z.enum(['campaign', 'adset', 'ad']).default('campaign'),
  entityIds: z.array(z.string()).optional(), // Compare specific entities, or all if not provided
  metrics: z.array(z.enum(['impressions', 'clicks', 'spend', 'ctr', 'cpc', 'cpm', 'purchase_roas'])).default(['ctr', 'cpc', 'purchase_roas']),
  rankBy: z.enum(['ctr', 'cpc', 'purchase_roas', 'spend', 'impressions']).default('ctr'),
  includeVideoMetrics: z.boolean().default(false),
  limit: z.number().min(1).max(50).default(10) // Top N entities to return
});
```

Tool definition:
- name: 'compare-entities'
- description: 'Compare and rank multiple campaigns, ad sets, or ads on performance metrics to identify top and bottom performers'
- Manual inputSchema

Implementation:
1. Initialize MetricsService
2. Build metric fields from schema
3. Query insights:
   ```typescript
   const params = {
     date_preset: args.dateRange,
     level: args.level,
     time_increment: 'all_days'
   };
   const data = await metrics.getAccountInsights(fields, params);
   ```
4. Filter to entityIds if provided
5. Parse all metrics (use parsers for ROAS, video, etc.)
6. For each metric in args.metrics:
   - Use rankByMetric() to rank entities
   - Use calculateMetricStats() for context
7. Rank overall by args.rankBy metric
8. Limit to top N using args.limit
9. Use identifyBestPerformers() and identifyWorstPerformers() for insights
10. Format response:
    ```json
    {
      "dateRange": "last_7d",
      "level": "campaign",
      "rankBy": "ctr",
      "totalEntities": 23,
      "showing": 10,
      "rankings": [
        {
          "rank": 1,
          "id": "...",
          "name": "...",
          "metrics": {
            "impressions": 12500,
            "clicks": 450,
            "ctr": 3.60,
            "cpc": 0.32,
            "spend": 144.00,
            "purchase_roas": 4.2
          },
          "percentiles": {
            "ctr": 95,
            "cpc": 82,
            "purchase_roas": 78
          }
        },
        // ... more entities
      ],
      "stats": {
        "ctr": { "min": 1.2, "max": 3.6, "mean": 2.4, "median": 2.3 },
        "cpc": { "min": 0.28, "max": 1.45, "mean": 0.62, "median": 0.54 }
      },
      "topPerformers": {
        "ctr": ["Campaign A", "Campaign B", "Campaign C"],
        "purchase_roas": ["Campaign D", "Campaign A", "Campaign E"]
      },
      "bottomPerformers": {
        "ctr": ["Campaign X", "Campaign Y", "Campaign Z"]
      }
    }
    ```
11. Return JSON.stringify(result, null, 2)

Error handling:
- If no entities match filters, return "No entities found for criteria"
- If limit > total entities, show all
- If rankBy metric missing for all entities, return error with suggestion

Why this format: Rankings make competitive positioning clear. Percentiles show relative performance. Stats provide benchmarks. Top/bottom lists highlight extremes.

What to avoid: Don't exclude entities with incomplete metrics (show with null values). Don't auto-filter by spend threshold (user decides). Don't aggregate across levels (campaign ≠ ad set).
  </action>
  <verify>npm run build succeeds, ranking utility integrated, multiple entities compared, sorting correct</verify>
  <done>Entity comparison tool implemented, ranking functional, top/bottom performers identified</done>
</task>

<task type="auto">
  <name>Task 3: Register comparison tool and complete Phase 4</name>
  <files>src/tools/index.ts, src/index.ts</files>
  <action>
Register compare-entities tool to complete Phase 4 comparative reports suite.

In src/tools/index.ts:
1. Import compareEntitiesTool from './compare-entities.js'
2. Add to tools array (9 tools total)

In src/index.ts:
1. Import compareEntities from './tools/compare-entities.js'
2. Add case to CallToolRequestSchema handler:
   ```typescript
   case 'compare-entities': {
     const result = await compareEntities(args as any);
     return { content: [{ type: 'text', text: result }] };
   }
   ```

Verification:
- npm run build succeeds
- npm run dev starts without errors
- 9 tools discoverable

Phase 4 complete deliverables:
- Time period comparison (04-01)
- Cross-entity comparison (04-02)
- 2 comparative analysis tools
- Comparison and ranking utilities

Why this completes Phase 4: ROADMAP.md goal "Generate week-over-week and campaign comparison reports" fully achieved. Time period comparison covers week-over-week, entity comparison covers campaign vs campaign.
  </action>
  <verify>npm run build && npm run dev succeeds, 9 tools total, Phase 4 goals achieved</verify>
  <done>Entity comparison tool registered, Phase 4 complete, full comparative reporting suite operational</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds with no TypeScript errors
- [ ] Ranking utility sorts correctly
- [ ] Percentile calculations accurate
- [ ] Top/bottom performers identified correctly
- [ ] Stats calculations (mean, median, stdDev) mathematically correct
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Phase 4 complete: Comparative reporting fully implemented
- 9 MCP tools total operational
- Week-over-week and campaign comparison available
- Ranking and statistical analysis functional
- Foundation ready for Phase 5 (Anomaly Detection)
</success_criteria>

<output>
After completion, create `.planning/phases/04-comparative-reports/04-02-SUMMARY.md`:

# Phase 4 Plan 02: Entity Comparison and Ranking Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `src/lib/ranking.ts` - Ranking and statistical utilities
- `src/tools/compare-entities.ts` - Entity comparison tool
- `src/tools/index.ts` - Tool registration
- `src/index.ts` - Handler registration

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Deviations from Plan

[Deviations or "None"]

## Next Phase Readiness

Phase 4 complete! Ready for Phase 5: Anomaly Detection.

Comparative reporting suite complete with:
- Time period comparison (04-01)
- Entity ranking and comparison (04-02)

Next phase will build on comparison utilities to detect performance anomalies automatically, surfacing significant deviations from baselines and trends.

## Verification Checklist

- [ ] All verification items from plan
</output>
