---
phase: 04-comparative-reports
plan: 01
type: execute
---

<objective>
Create week-over-week comparison tool for identifying performance trends and changes.

Purpose: Enable Claude to answer questions like "How did my campaigns perform this week vs last week?" and "Which metrics improved/declined?" by comparing two time periods and calculating deltas.

Output: compare-time-periods MCP tool that queries two date ranges, calculates percent changes for all metrics, and highlights significant improvements or declines.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-metrics-query/DISCOVERY.md
@.planning/phases/02-core-metrics-query/02-01-SUMMARY.md
@.planning/phases/03-video-analytics/03-03-SUMMARY.md

# Key files
@src/meta/metrics.ts
@src/lib/parsers.ts
@src/tools/get-campaign-performance.ts
@src/tools/index.ts
@src/index.ts

**Tech stack available:** MetricsService, all parsers, 7 existing MCP tools

**Established patterns:**
- MetricsService for insights queries
- Pretty-printed JSON responses
- Manual inputSchema for tools

**Constraining decisions:**
- Plan 02-01: MetricsService supports date_preset and custom time_range
- Plan 02-02: Metric fields established (impressions, clicks, spend, CTR, CPC, ROAS)
- DISCOVERY.md: time_range accepts { since, until } in YYYY-MM-DD format
- Phase 3: Video metrics and engagement analysis available

**Issues being addressed:** None
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create time period comparison utility</name>
  <files>src/lib/comparison.ts</files>
  <action>
Create utility module for comparing metrics between two time periods.

Export functions:

1. calculateDelta(current, previous, metricName):
   - Absolute delta: current - previous
   - Percent change: ((current - previous) / previous) * 100
   - Handle special cases:
     - If previous = 0 and current > 0: Return "+âˆž" or "+100%" with note
     - If previous > 0 and current = 0: Return "-100%"
     - Both zero: Return "0%"
   - Return: { absolute, percent, direction: 'up' | 'down' | 'unchanged' }

2. classifyChange(percentChange, metricType):
   - For "better when higher" metrics (CTR, ROAS, completions):
     - Significant improvement: >10%
     - Minor improvement: 5-10%
     - Unchanged: -5% to 5%
     - Minor decline: -10% to -5%
     - Significant decline: <-10%
   - For "better when lower" metrics (CPC, CPM):
     - Invert classification
   - For neutral metrics (impressions, spend):
     - Just report direction
   - Return classification string

3. compareMetricSets(currentMetrics, previousMetrics):
   - Accept two metric objects
   - Calculate deltas for all common metrics
   - Return comparison object with all deltas and classifications
   - Sort by absolute percent change (largest changes first)

4. formatPercentChange(percentChange):
   - Format with sign: "+15.3%" or "-8.7%"
   - Handle edge cases (infinity, very small changes)
   - Return formatted string

Add TypeScript interfaces for comparison results.
Add JSDoc with examples.

Why separate module: Reusable comparison logic. Can be used by week-over-week, campaign comparison, and future anomaly detection tools.

What to avoid: Don't make value judgments in classifications ("good" vs "bad" - use "improvement" vs "decline" relative to metric type). Don't filter small changes (user decides significance threshold).
  </action>
  <verify>npm run type-check passes, comparison functions exported, edge cases handled</verify>
  <done>Comparison utility module created with delta calculation, change classification, and formatting functions</done>
</task>

<task type="auto">
  <name>Task 2: Create compare-time-periods MCP tool</name>
  <files>src/tools/compare-time-periods.ts</files>
  <action>
Create MCP tool for week-over-week and custom period comparisons.

Zod schema:
```typescript
const schema = z.object({
  level: z.enum(['campaign', 'adset', 'ad']).default('campaign'),
  entityId: z.string().optional(),
  currentPeriod: z.object({
    since: z.string().regex(/^\d{4}-\d{2}-\d{2}$/),
    until: z.string().regex(/^\d{4}-\d{2}-\d{2}$/)
  }),
  previousPeriod: z.object({
    since: z.string().regex(/^\d{4}-\d{2}-\d{2}$/),
    until: z.string().regex(/^\d{4}-\d{2}-\d{2}$/)
  }),
  metrics: z.array(z.enum(['impressions', 'clicks', 'spend', 'ctr', 'cpc', 'cpm', 'purchase_roas'])).default(['impressions', 'clicks', 'spend', 'ctr', 'cpc']),
  includeVideoMetrics: z.boolean().default(false)
});
```

Alternatively, support preset comparisons:
```typescript
  comparisonType: z.enum(['week-over-week', 'month-over-month', 'custom']).optional()
  // If week-over-week: Auto-calculate current = last 7 days, previous = prior 7 days
  // If month-over-month: current = this month, previous = last month
```

Tool definition:
- name: 'compare-time-periods'
- description: 'Compare campaign/adset/ad performance between two time periods to identify trends, improvements, and declines'
- Manual inputSchema

Implementation:
1. Initialize MetricsService
2. Build metric fields from schema (add video fields if includeVideoMetrics=true)
3. Query current period:
   ```typescript
   const params1 = {
     time_range: args.currentPeriod,
     level: args.level,
     time_increment: 'all_days'
   };
   const currentData = await metrics.getAccountInsights(fields, params1);
   ```
4. Query previous period (same params, different time_range)
5. Filter both datasets by entityId if provided
6. For each entity in results:
   - Match by ID between current and previous
   - Parse metrics (use parsers for ROAS, video, etc.)
   - Use compareMetricSets() from comparison utility
   - Include raw values and deltas
7. Format response:
   ```json
   {
     "comparison": {
       "current": { "since": "2026-01-24", "until": "2026-01-30" },
       "previous": { "since": "2026-01-17", "until": "2026-01-23" }
     },
     "level": "campaign",
     "results": [
       {
         "id": "...",
         "name": "...",
         "current": {
           "impressions": 12500,
           "clicks": 342,
           "ctr": 2.74,
           "spend": 145.67
         },
         "previous": {
           "impressions": 10800,
           "clicks": 298,
           "ctr": 2.76,
           "spend": 132.40
         },
         "changes": {
           "impressions": { "absolute": 1700, "percent": "+15.7%", "direction": "up" },
           "clicks": { "absolute": 44, "percent": "+14.8%", "direction": "up" },
           "ctr": { "absolute": -0.02, "percent": "-0.7%", "classification": "unchanged" },
           "spend": { "absolute": 13.27, "percent": "+10.0%", "direction": "up" }
         },
         "significantChanges": [
           { "metric": "impressions", "change": "+15.7%", "classification": "improvement" },
           { "metric": "clicks", "change": "+14.8%", "classification": "improvement" }
         ]
       }
     ]
   }
   ```
8. Return JSON.stringify(result, null, 2)

Error handling:
- If entity exists in one period but not other, note "New" or "Paused" status
- If date ranges overlap, warn "Periods overlap - comparison may be misleading"
- If no data in either period, return appropriate message

Why this format: Side-by-side comparison makes trends obvious. Significant changes highlighted for quick scanning. Classifications help Claude interpret.

What to avoid: Don't assume period lengths are equal (user might compare 7 days vs 30 days). Don't filter entities that exist in only one period (new/paused campaigns are important). Don't aggregate across entities (show each separately).
  </action>
  <verify>npm run build succeeds, comparison utility integrated, delta calculations correct, both periods queried</verify>
  <done>Time period comparison tool implemented, week-over-week analysis functional, trend identification working</done>
</task>

<task type="auto">
  <name>Task 3: Register comparison tool in MCP server</name>
  <files>src/tools/index.ts, src/index.ts</files>
  <action>
Register compare-time-periods tool in MCP server.

In src/tools/index.ts:
1. Import compareTimePerio dsTool from './compare-time-periods.js'
2. Add to tools array (8 tools total)

In src/index.ts:
1. Import compareTimePeriods from './tools/compare-time-periods.js'
2. Add case to CallToolRequestSchema handler:
   ```typescript
   case 'compare-time-periods': {
     const result = await compareTimePeriods(args as any);
     return { content: [{ type: 'text', text: result }] };
   }
   ```

Verification:
- npm run build succeeds
- npm run dev starts without errors
- 8 tools discoverable

Why this pattern: Consistent with all prior tool registrations. Comparison as its own tool enables flexible time period analysis.
  </action>
  <verify>npm run build && npm run dev succeeds, 8 tools registered, time period comparison operational</verify>
  <done>Comparison tool registered, week-over-week analysis queryable through MCP</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds with no TypeScript errors
- [ ] Comparison utility handles edge cases (zero values, infinity)
- [ ] Delta calculations mathematically correct
- [ ] Both time periods queried successfully
- [ ] Classifications appropriate for metric types (higher/lower is better)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Week-over-week comparison functional
- Custom period comparisons supported
- Trend identification working
- Foundation ready for 04-02 (campaign comparison)
</success_criteria>

<output>
After completion, create `.planning/phases/04-comparative-reports/04-01-SUMMARY.md`:

# Phase 4 Plan 01: Time Period Comparison Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `src/lib/comparison.ts` - Comparison utility functions
- `src/tools/compare-time-periods.ts` - Time period comparison tool
- `src/tools/index.ts` - Tool registration
- `src/index.ts` - Handler registration

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Deviations from Plan

[Deviations or "None"]

## Next Phase Readiness

Ready for 04-02-PLAN.md (Campaign Comparison). Time period comparison enables trend analysis. Next plan will add cross-campaign comparison for competitive analysis within account.

## Verification Checklist

- [ ] All verification items from plan
</output>
